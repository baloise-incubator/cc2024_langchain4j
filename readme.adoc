= LLM for engineering questions using langchain4j (optionally on Azure)

== Vision

* Questions about code (preferrably our company repositories) can be answered
** similar to co-pilot
** more specific to our company
** eg 'how to search for a partner?', how to use the partner service, ...

=== Monday
* LangChain4J learnings
** RAG (Retrieval Augmented Generation) Process
** Step 1: ("setup time") create embedded Model of own documents (e.g. Java files)
** Step 2: ("runtime") ask send question tor embeddings. -> 1-n results
** Step 3: ("runtime") send result(s) and question to LLM

we used OpenAI


=== Tuesday

* Ollama (local LLM instead of OpenAI)
** ollama in local docker container (with LLMs: 'codegemmaa', codellama'), and natively on Mac
** used ollama-ui as a chat-gui (very similar to chatGPT UI) against ollama server
* Inspect copilot protocol (which requests are sent from VS-Code to github) using an nginx proxy with logging
* Redirect copilot requests to local ollama server via ollama-copilat (go program)


=== Wednesday
* Tried to create embeddings for local Java files -> results not very promising
** -> we need a language (Java) specific splitter. Not available in langchain4J but in langchain-ai (Python)

=== Thursday
* langchain-ai with openai (pay)
** index Java codebase -> works much better



=== ToDo

* ci/cd pipeline for such an application
* how can we offer the service sustainably
* learning, learning
* TDD such an app

=== Done
* Checked out Ollama https://docs.langchain4j.dev/integrations/language-models/ollama
* explored the https://github.com/langchain4j/langchain4j-examples/tree/main/other-examples/src/main/java[langchain4j example repository]
* found the https://github.com/langchain4j/langchain4j-examples/blob/main/other-examples/src/main/java/embedding/store/InMemoryEmbeddingStoreExample.java[InMemoryEmbeddingStoreExample] which
** load files/text into the model ("I like football", ...)
** and ask questions "which sport do you like" -> "football"

* watched very good presentation on an RAG use case: https://youtu.be/J-3n7xs98Kc?si=xiSmOOrVC3IcbxEP
** 25:05: interface EmbeddingModel: AllMiniLmL6V2EmbeddingModel vs. OpenAIModel
** 25:56: EmbeddingStore
** 27:50: Retriever
** 28:15: content ingestion explained
** 28:37:  workaround CommandlineRunner
** 29:45:  load document
** 32:35:  "this is RAG", he used it to build the vaadin docs assistant
** 34:30:  tools
** github:
*** https://github.com/marcushellberg/java-ai-playground
*** https://github.com/marcushellberg/java-chat-with-documents

* see also 3blue1brown series: https://www.youtube.com/watch?v=wjZofJX0v4M&t=2s

* RAG for code: https://cloud.google.com/blog/products/ai-machine-learning/context-aware-code-generation-rag-and-vertex-ai-codey-apis?hl=en

=== Links

* https://www.youtube.com/watch?v=AAMJZTEH_h4&t=236s[Talk to Your Code | Github Repo | Learn How GitHub Co-Pilot & Others Transform Coding]
* https://www.youtube.com/watch?v=aD-u0gl93wM&t=5s[CODE-LLAMA For Talking to Code Base and Documentation]

